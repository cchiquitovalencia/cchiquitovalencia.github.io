---
title: "Entendiendo la matriz de confusión"
description: "TRADING Serie - Parte 2"
author:
  - name: Cristian Chiquito Valencia
    url: https://cchiquitovalencia.github.io/
    affiliation: Independent @ CHV
date: 04-08-2025
categories: [Machine Learning, Trading, Regresión Logística] # self-defined categories
citation: 
  url: https://cchiquitovalencia.github.io/posts/2025-04-08-predictions_on_stocks2/
image: confusion.jpeg
draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
---

La matriz de confusión tiene las claves para entender cualquier modelo de machine learning, una mina de oro con 25 métricas, pero hay 7 de ellas que son especialmente importantes: **recall** (sensibilidad), **specificity** (especificidad), el **índice J de Youden**, **precision** (precisión), **NPV** (Valor Negativo Predicho), **accuracy** (exactitud) y **balanced accuracy** (exactitud equilibrada). En este post, exploramos qué hace que estos siete sean esenciales, cómo funcionan y por qué son la columna vertebral del aprendizaje automático.

Usaremos los datos de este [post](https://cchiquitovalencia.github.io/posts/2025-04-07-predictions_on_stocks/ "Modelando stocks financieros"). Lo que he hecho es crear un paquete de R para que puedas instalarlo con el siguiente comando:

```{r, eval=FALSE}
devtools::install_github("cchiquitovalencia/myfinance")
```

Si obtienes algo parecido a esto, estás listo para continuar:

[![Detalle de instalación del paquete myfinance](images/myfinance_devtools.png){#fig-1 fig-align="center" width="670"}](https://github.com/cchiquitovalencia/myfinance/tree/master)

A grandes rasgos lo que hace este paquete es replicar el flujo de trabajo visto en la primera parte de la serie, para que usar los mismos datos en la evaluación del modelo. Si te sientes un poco aventurer\@ puedes crear un branch para modificar la función `main_analysis()` y aplicar el flujo a otros datos, no tiene que ser el **DJI**, puedes usar el que desees.

Y con el siguiente código ya tendras el resultado del modelo de regresión logística.

```{r, message=FALSE, warning=FALSE}
library(myfinance)
library(caret)

# Función para calcular los resultados tanto de train como de test
calculate_results <- function(model, data) {
  # Realizar predicción
  predictions <- predict(model, data)
  
  # Convertir predicciones en probabilidades
  probabilities <- 1 / (1 + exp(-predictions))
  
  # Determinar la dirección basada en el umbral de 0.5
  direction <- ifelse(probabilities > 0.5, 1, 0)
  
  # Crear la matriz de confusión
  confusion <- confusionMatrix(factor(direction), factor(data$Direction), mode = "everything")
  
  # Devolver los resultados
  list(
    predicted_direction = direction,
    confusion_matrix = confusion,
    predictions = predictions
  )
}

# Ejecutar el análisis principal
modelo <- main_analysis()

# Calcular resultados para train
train_results <- calculate_results(modelo$modelo, modelo$normalized$train)
train_confusion <- train_results$confusion_matrix

# Calcular resultados para test
test_results <- calculate_results(modelo$modelo, modelo$normalized$test)
test_confusion <- test_results$confusion_matrix
```

Revisamos resultados de los datos de entrenamiento del modelo:

```{r}
train_confusion
```

```{r}
test_confusion
```

Un problema que debemos resolver desde un principio es que estas métricas tienen múltiples nombres dependiendo del campo en el que te encuentres. Por ejemplo, la sensibilidad se llama **recall** en aprendizaje automático, **tasa de positivos verdaderos** en medicina y **probabilidad de detección** en ingeniería. Es la misma cosa, pero los nombres cambian dependiendo de con quién estés hablando, y eso puede ser confuso.

Para calcular la sensibilidad, solo necesitamos la columna izquierda de la matriz de confusión. Dividimos los verdaderos positivos entre todos los casos positivos reales. En otras palabras, la sensibilidad mide el porcentaje de verdaderos positivos que el modelo identifica correctamente. Por eso, el segundo nombre para la sensibilidad es **tasa de positivos verdaderos**.

$$
Sensibilidad=TPR=\frac{TP}{TP+FN}
$$

En nuestro ejemplo del DJI, se trata del porcentaje de subidas de la acción que nuestro modelo detectó, lo que explica la tercera definición: **probabilidad de detección**. La sensibilidad oscila entre 0 y 1, o entre 0% y 100%, donde 1 indica una sensibilidad perfecta (sin falsos negativos) y 0 significa que no se puede detectar ningún caso positivo.

En nuestro ejemplo, la sensibilidad es alta `r scales::percent(train_confusion$byClass[1], accuracy = 0.01)` porque detectamos a `r train_confusion$table[1]` subidas de precio, y una alta sensibilidad es lo que usualmente queremos. Especialmente en medicina, la sensibilidad es crucial para pruebas de detección como la detección de cáncer o enfermedades infecciosas como COVID-19. Si una prueba tiene baja sensibilidad, significa que muchos pacientes con cáncer son mal diagnosticados como sanos, lo que lleva a retrasos peligrosos en el tratamiento. Una prueba con alta sensibilidad asegura que la mayoría de los pacientes con la condición son identificados, incluso si genera algunos falsos positivos, que pueden ser seguidos con pruebas más específicas.

En aprendizaje automático, la sensibilidad es esencial para la detección de fraude o la detección de anomalías, donde los casos positivos, como las transacciones fraudulentas con tarjetas de crédito, son *raros pero críticos*. Un modelo con alta sensibilidad asegura que la mayoría de las actividades fraudulentas son detectadas, incluso si genera algunas falsas alarmas.

Sí, podemos maximizar la sensibilidad aumentando el costo de los falsos negativos con el paquete **`cutpointr`** y la función **`cutpoint()`**.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
d2 <- modelo$normalized$test |> 
    as.data.frame() |> 
    mutate(pred_probs = test_results$predictions,
           pred_classes = factor(test_results$predicted_direction))
    

library(cutpointr, verbose = FALSE)
sensitive_case <- cutpointr(
    data = d2,
    x = pred_probs,
    class = Direction,
    method = minimize_metric,
    metric = misclassification_cost,
    cost_fp = 1,
    cost_fn = 5
)

sensitive_case |> 
    t()

sensitive_case |> 
    plot_roc()
```

Por ejemplo, si establecemos el costo de los falsos negativos en cinco veces el costo de los falsos positivos, podemos aumentar la sensibilidad de un `r scales::percent(train_confusion$byClass[1], accuracy = 0.01)` a un asombroso `r scales::percent(sensitive_case$sensitivity, accuracy = 0.01)`.

La sensibilidad nos ayuda a detectar tantos positivos verdaderos como sea posible, lo cual es crucial para escenarios como la detección del cáncer, pero aquí está el problema: aumentar demasiado la sensibilidad a menudo aumenta los falsos positivos, lo que puede ser peligroso en otros casos, como en la conducción autónoma, donde un coche autónomo podría confundir una sombra con un peatón, activando una frenada de emergencia innecesaria y un riesgo de accidente. En situaciones donde minimizar los falsos positivos y maximizar los verdaderos negativos es lo que más importa, una alta **especificidad** es clave. Así que hablemos de la especificidad.

Desafortunadamente, la especificidad también tiene muchos nombres. En medicina, se le llama **tasa de negativos verdaderos**; en aprendizaje automático, a veces se la conoce como **selectividad**; en ingeniería, algunos la llaman **probabilidad de predicción correcta**, un término que personalmente encuentro particularmente intuitivo. Para calcular la especificidad, solo necesitamos la columna derecha de la matriz de confusión. Dividimos los verdaderos negativos entre todos los casos negativos reales.

$$
Especificidad=TNR=\frac{TN}{TN+FP}
$$

En palabras sencillas, la especificidad mide el porcentaje de negativos reales que el modelo identifica correctamente como negativos. Por eso, otro nombre para la especificidad es la **tasa de negativos verdaderos**. En nuestro ejemplo, la especificidad nos dice **cuántos no sobrevivientes** el modelo clasificó correctamente como tales. La especificidad oscila entre 0 y 1, o entre 0% y 100%, donde 1 indica un modelo perfecto, sin falsos positivos, y 0 significa que no puede descartar los casos negativos en absoluto.

En nuestro ejemplo, la especificidad es sólida, con un `r scales::percent(train_confusion$byClass[2], accuracy = 0.01)`, porque solo etiquetamos incorrectamente a `r train_confusion$table[3]` bajadas como subidas, **mientras que la sensibilidad se centra en los positivos, la especificidad se centra en evitar las falsas alarmas, lo cual es crucial en ciertos escenarios de alto riesgo**. Por ejemplo, en medicina, una prueba con baja especificidad podría etiquetar falsamente a personas sanas como tener cáncer, lo que lleva a ansiedad innecesaria, biopsias, quimioterapia o incluso cirugía. Además, una baja especificidad en una prueba de una enfermedad rara puede abrumar a los sistemas de salud con falsas alarmas. Por eso, a veces necesitamos maximizar la especificidad y mantener los falsos positivos tan bajos como sea posible.

Sí, podemos hacerlo aumentando el costo de los falsos positivos usando el paquete **cutpointr** en R. Podemos desplazar el umbral de decisión para minimizar los falsos positivos y priorizar los verdaderos negativos, incluso si eso significa perder algunos verdaderos positivos.

```{r}
sensitive_case <- cutpointr(
    data = d2,
    x = pred_probs,
    class = Direction,
    method = minimize_metric,
    metric = misclassification_cost,
    cost_fp = 5,
    cost_fn = 1
)

sensitive_case |> 
    t()

sensitive_case |> 
    plot_roc()
```

Por ejemplo, si establecemos el costo de los falsos positivos en cinco veces el costo de los falsos negativos, podemos hacer que la especificidad pase de un impresionante `r scales::percent(train_confusion$byClass[2], accuracy = 0.01)` a un increíble `r scales::percent(sensitive_case$specificity, accuracy = 0.01)`.

Como puedes ver, los científicos de datos enfrentan un desafío complejo: aumentar la sensibilidad para detectar todos los positivos posible corre el riesgo de aumentar las falsas alarmas, mientras que aumentar la especificidad para proteger las transacciones legítimas podría significar perder algunas actividades fraudulentas. Es aquí donde entra en juego el **índice J de Youden**. El índice J de Youden ayuda a encontrar un punto óptimo entre sensibilidad y especificidad, maximizando la efectividad general del modelo. Así que hablemos de eso.

**El índice J de Youden es increíblemente útil cuando tanto los falsos positivos como los falsos negativos tienen consecuencias significativas**. Es por eso que el índice J de Youden maximiza los verdaderos positivos y minimiza los falsos positivos. Un dato curioso: el índice J de Youden también se conoce como **información basada en corredores de apuestas** porque mide cuánto más informado está una prueba o modelo en comparación con el azar. El término proviene del mundo de las apuestas, donde los corredores de apuestas dependen de la información para predecir los resultados mejor que el azar.

Para calcular el índice J de Youden, combinamos sensibilidad y especificidad. Dado que la suma simple de sensibilidad y especificidad oscila entre 1 y 2, lo que no es fácil de interpretar, el menos uno en la fórmula de J normaliza la escala para que oscile entre 0 y 1, lo que es fácil de interpretar.

$$
J=Sensibilidad+Especificidad-1
$$

Específicamente, J igual a 0 indica que la prueba no funciona mejor que el azar y, por lo tanto, no tiene información útil. J igual a 1 indica que la prueba distingue perfectamente entre positivos y negativos. En resumen, el índice J de Youden o **información** nos dice qué tan bien una prueba o modelo mejora la toma de decisiones en comparación con el azar.

En R, hay dos formas fáciles de calcular el índice J de Youden: una es usando la función **youden** del paquete **cutpointr**,

```{r}
indexJ <- youden(tp = train_confusion$table[1],
       tn = train_confusion$table[4],
       fp = train_confusion$table[3],
       fn = train_confusion$table[2])

indexJ
```

y la otra es usando el resumen de la función **epi.tests** del paquete **epiR**:

```{r}
summary(epiR::epi.tests(train_confusion$table)) |> 
    gt::gt() |> 
    gt::tab_style(
    style = list(
      gt::cell_fill(color = "orange"),
      gt::cell_text(weight = "bold")
      ),
    locations = gt::cells_body(
      columns = statistic,
      rows = statistic >= "youden"
    )
  )
```

Un $J$ de `r scales::percent(indexJ, accuracy = 0.01)` es fuerte. Nos dice que el modelo funciona mejor que el azar, pero podría ser perfeccionado para equilibrar mejor la sensibilidad y la especificidad. Si te preguntas cómo maximizar el índice J de Youden, la respuesta es usando la misma función **cutpointr** que usamos para maximizar la sensibilidad y la especificidad.

```{r}
improve_j <- cutpointr(
    data = d2,
    x = pred_probs,
    class = Direction,
    method = maximize_metric,
    metric = youden,
    boot_runs = 1000
)

improve_j |> t()

improve_j |> 
    plot_metric()
```

Lo que quiero mostrarte es que, aumentando ligeramente la sensibilidad del `r scales::percent(train_confusion$byClass[1], accuracy = 0.01)` al `r scales::percent(improve_j$sensitivity, accuracy = 0.01)` y disminuyendo ligeramente la especificidad del `r scales::percent(train_confusion$byClass[2], accuracy = 0.01)` al `r scales::percent(improve_j$specificity, accuracy = 0.01)`, podríamos aumentar el índice J de Youden del `r scales::percent(indexJ, accuracy = 0.01)` al `r scales::percent(improve_j$youden, accuracy = 0.01)`. ¡Genial!, lo que hace que nuestro modelo sea más preciso.

Y hablando de precisión, debemos discutirla a continuación, porque la precisión es una de las métricas más importantes y ampliamente utilizadas derivadas de la matriz de confusión. La precisión, también conocida como **valor predictivo positivo**, mide qué tan precisas son las predicciones positivas. Es el porcentaje de resultados positivos que son correctos, lo que muestra qué tan probable es que un resultado positivo sea verdadero.

Durante los primeros días de la pandemia de COVID-19, la sensibilidad fue clave para detectar a tantas personas infectadas como posible, pero más tarde, cuando la propagación se desaceleró y las pruebas se multiplicaron, la precisión se convirtió en el foco de atención.

Para calcular la precisión, nos centramos en la primera fila de la [matriz de confusión](https://cchiquitovalencia.github.io/posts/2025-04-07-predictions_on_stocks/#fig-1), los positivos predichos. Es la proporción de verdaderos positivos entre todos los casos positivos predichos. La precisión oscila entre 0 y 1, o entre 0% y 100%. Un puntaje de 1 significa que todas las predicciones positivas fueron correctas, sin falsos positivos, mientras que 0 significa que todas fueron incorrectas.

$$
Precisión=PPV=\frac{TP}{TP+FP}
$$

En nuestro ejemplo, una precisión del `r scales::percent(train_confusion$byClass[3], accuracy = 0.01)` nos dice que el `r scales::percent(train_confusion$byClass[3], accuracy = 0.01)` de las acciones que el modelo etiquetó como subidas realmente subieron. Eso es sólido, ya que solo `r train_confusion$table[3]` de los `r (train_confusion$table[3] + train_confusion$table[1])` movimientos hacia arriba predichos fueron mal clasificados.

Pero, ¿por qué la precisión es tan importante? Piensa en los filtros de correo no deseado. Una alta precisión asegura que cuando un correo aterriza en tu bandeja de spam, es casi seguro que es basura. Por otro lado, si tienes una baja precisión, te encontrarás revolviendo spam para encontrar ese correo urgente de tu jefe. Es un infierno, ¿no? La precisión también se puede calcular usando sensibilidad, especificidad y prevalencia, aunque idealmente, es un poco pesado y difícil de recordar.

$$
PPV=\frac{Sensibilidad*Prevalencia}{Sensitividad*Prevalencia+(1-Especificidad)*(1-Prevalencia)}
$$

Lo que es fácil de recordar es que cuando hay un valor predictivo positivo, también debe haber un valor predictivo negativo. De hecho, el valor predictivo negativo nos dice cuántos resultados de prueba negativos son precisos.

Para calcular el **valor predictivo negativo (VPV)**, solo necesitamos la fila inferior de la matriz de confusión, los negativos predichos. Dividimos el número de verdaderos negativos entre el total de predicciones negativas. El valor predictivo negativo es crucial porque construye confianza en los resultados negativos.

$$
NPV=\frac{TN}{TN+FN}
$$

Por ejemplo, en la detección del cáncer, si tu prueba da negativo y el VPV es del 99%, hay un 99% de probabilidad de que realmente no tengas cáncer. Eso es muy tranquilizador. Solo hay un 1% de posibilidad de que haya un error. Pero si el VPV es del `r scales::percent(train_confusion$byClass[4], accuracy = 0.01)`, como en nuestro ejemplo, significa que el `r scales::percent(train_confusion$byClass[4], accuracy = 0.01)` de las veces un resultado negativo es correcto, pero que hay un `r scales::percent(1-train_confusion$byClass[4], accuracy = 0.01)` de probabilidad de que el resultado negativo sea incorrecto y que realmente tengas cáncer. Eso probablemente no te tranquilizaría mucho, ya que es un riesgo bastante grande. Por eso, el VPV es realmente importante porque afecta directamente cuánto puedes confiar en un resultado negativo.

El valor predictivo positivo nos dice cuántas de nuestras predicciones positivas son realmente correctas, mientras que el valor predictivo negativo mide cuántas de nuestras predicciones negativas son precisas. Ambas son útiles, pero se centran solo en un lado de los resultados de las predicciones, ya sean positivos o negativos. Pero, **¿qué pasa si necesitamos evaluar la corrección general de todas las predicciones, tanto positivas como negativas?**

Ahí es donde entra en juego la **exactitud**. La exactitud nos da una visión general del rendimiento midiendo la proporción de todas las predicciones correctas, independientemente de la clase. De hecho, ¿qué métricas usas y cuáles son las principales en tu opinión? Realmente quiero saber tus pensamientos. La fórmula para la exactitud es sencilla: suma los verdaderos positivos y los verdaderos negativos, y divide entre el número total de casos.

$$
Exactitud=\frac{TP+TN}{total}
$$

Eso es todo, bastante sencillo, ¿verdad? Mientras que una mayor exactitud es generalmente lo que queremos, la exactitud **puede ser engañosa al lidiar con clases ligeramente desequilibradas,** donde un resultado sobrepasa ligeramente al otro. Es por eso que tenemos una métrica más robusta como la **exactitud equilibrada**.

$$
\text{Exactitud equilibrada}=\frac{Sensibilidad+Especificidad}{2}
$$

La exactitud equilibrada ofrece una solución promediando la sensibilidad y la especificidad, asegurando que ambas clases reciban una consideración igual. En nuestro ejemplo, una exactitud equilibrada del `r scales::percent(train_confusion$byClass[11], accuracy = 0.01)` está solo `r scales::percent(train_confusion$overall[1] - train_confusion$byClass[11], accuracy = 0.01)` por debajo de la exactitud general del `r scales::percent(train_confusion$overall[1], accuracy = 0.01)`, lo que sugiere que las clases están relativamente equilibradas, pero eso no es siempre el caso. Por eso, no confío en la exactitud si no veo una matriz de confusión o una exactitud equilibrada comparada con la exactitud. Podemos calcular fácilmente la exactitud equilibrada usando la función de `confusionMatrix()` del paquete **`caret`**.

Mientras que la exactitud equilibrada es útil para conjuntos de datos ligeramente desequilibrados, a veces la exactitud equilibrada puede ser casi la mitad de la exactitud, lo que indica que nuestro conjunto de datos está muy sesgado y no balanceado.

```{r}
casos <- 10000
prop_positivos <- 0.05
cant_positivos <- casos * prop_positivos
cant_negativos <- casos - cant_positivos

actual <- c(rep(1, cant_positivos), rep(0, cant_negativos))
predicciones <- rep(0, casos)

ejemplo <- confusionMatrix(factor(predicciones), factor(actual))

ejemplo
```

Por ejemplo, consideremos un conjunto de datos con `r casos` casos, donde solo el `r scales::percent(prop_positivos)` son positivos. En este caso, un modelo puede lograr una asombrosa exactitud del `r scales::percent(ejemplo$overall[1], accuracy = 0.01)` simplemente prediciendo negativo todas las veces, pero falla al detectar cualquier positivo, lo que resulta en un 100% de predicciones positivas incorrectas, lo cual es **inaceptable**. Sin ver la matriz de confusión, no somos conscientes de este desastre.

Sin embargo, la exactitud equilibrada de `r scales::percent(ejemplo$byClass[11], accuracy = 0.01)` revela esta debilidad al dar una puntuación mucho más baja, indicando que algo está seriamente mal. **Centrarse solo en la exactitud puede ocultar muchos errores**, lo que lleva a malas decisiones. A veces, **saber cuán impreciso es nuestro modelo puede ser muy útil porque revela dónde y cuántas veces nuestro modelo está equivocado**.

Métricas como la **tasa de malclasificación** y la **tasa de descubrimiento falso** son increíblemente prácticas y merecen su propio espacio de expliación. Por lo tanto, si quieres que tu modelo sea sólido y confiable, o si necesitas explicar por qué algunos resultados del modelo no deben confiarse, debes ver el siguiente [post](https://cchiquitovalencia.github.io/posts/2025-04-11-predictions_on_stocks3/).
