{
  "hash": "1b04f1365ea11e4e50ce6bceec08ea5a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"La ilusión de la precisión: ¿Cómo detectar los errores ocultos en tus datos?\"\ndescription: \"TRADING Serie - Parte 3\"\nauthor:\n  - name: Cristian Chiquito Valencia\n    url: https://cchiquitovalencia.github.io/\n    affiliation: Independent @ CHV\ndate: 04-11-2025\ncategories: [Machine Learning, Trading, Regresión Logística, Accuracy, Métricas de error, Matriz de confusión, Estadística] # self-defined categories\ncitation: \n  url: https://cchiquitovalencia.github.io/posts/2025-04-11-predictions_on_stocks3/\nimage: ilusion.jpeg\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n---\n\n\n\nHablemos de **precisión** durante un momento. Un modelo con alta precisión parece una victoria, ¿verdad? Pero aquí está el problema: la precisión puede ser engañosa. Trata cada predicción como igualmente importante, y a menudo oculta errores cuando los conjuntos de datos están **desequilibrados**.\n\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\nlibrary(caret)\ncasos <- 7560\nprop_positivos <- 0.03\ncant_positivos <- floor(casos * prop_positivos)\ncant_negativos <- casos - cant_positivos\n\nactual <- c(rep(1, cant_positivos), rep(0, cant_negativos))\npredicciones <- rep(0, casos)\n\nejemplo <- confusionMatrix(factor(predicciones), factor(actual))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in confusionMatrix.default(factor(predicciones), factor(actual)):\nLevels are not in the same order for reference and data. Refactoring data to\nmatch.\n```\n\n\n:::\n\n```{.r .cell-code}\nejemplo\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 7334  226\n         1    0    0\n                                         \n               Accuracy : 0.9701         \n                 95% CI : (0.966, 0.9738)\n    No Information Rate : 0.9701         \n    P-Value [Acc > NIR] : 0.5177         \n                                         \n                  Kappa : 0              \n                                         \n Mcnemar's Test P-Value : <2e-16         \n                                         \n            Sensitivity : 1.0000         \n            Specificity : 0.0000         \n         Pos Pred Value : 0.9701         \n         Neg Pred Value :    NaN         \n             Prevalence : 0.9701         \n         Detection Rate : 0.9701         \n   Detection Prevalence : 1.0000         \n      Balanced Accuracy : 0.5000         \n                                         \n       'Positive' Class : 0              \n                                         \n```\n\n\n:::\n:::\n\n\n\nImagina que estás diagnosticando una enfermedad donde solo el 3% de los casos son positivos. Un modelo podría obtener una precisión del 97.01% simplemente diciendo \"no hay enfermedad\" todo el tiempo.\n\n![Si el modelo predice \"no hay enfermedad\" cada vez.](imagen.png){#fig-1 fig-align=\"center\" width=\"195\"}\n\nSeguro, **técnicamente** está en lo correcto la mayoría de las veces, **pero falla por completo** al punto de encontrar esos **casos positivos críticos que realmente importan** (los **226** casos).\n\nAhí es donde entran las métricas de error. Métricas como la\n\n-   [Tasa de Error o Malclasificación] (**Misclassification Rate**)\n\n-   [Tasa de falsos positivos] (**FPR**)\n\n-   [Tasa de falsos negativos] (**FNR**)\n\n-   [Tasa descubrimiento falso] (**FDR**)\n\n-   [Tasa de omisión falsa] (**FOR**)\n\nllegan más a fondo. Muestran exactamente dónde lucha el modelo, ayudándonos a entender cómo maneja tanto las clases fáciles como las difíciles.\n\nAsí que la precisión no siempre es útil. **La clave real para construir mejores modelos y tomar decisiones más inteligentes radica en aceptar los errores**. Ahí es donde ocurre la magia.\n\nCada modelo de clasificación tiene un objetivo: cometer la menor cantidad de errores posibles. Ahí es donde entra la tasa de error, también llamada tasa de malclasificación.\n\n## Tasa de Error o Malclasificación\n\nEn términos sencillos, la tasa de error es la proporción de predicciones incorrectas. Para calcularla, sumamos los falsos positivos y los falsos negativos, y luego dividimos entre el número total de casos.\n\n$$\n\\text{Tasa de Error}=\\frac{FP+FN}{total}\n$$ {#eq-tasa_error}\n\nPiensa en los coches autónomos: la tasa de error muestra cuántas veces el coche toma una mala decisión, una métrica que podría significar la vida o la muerte.\n\nAquí tienes un truco útil: la tasa de error es simplemente lo contrario de la precisión. Resta la precisión de uno y ya tienes la tasa de error.\n\n$$\n\\text{Tasa de Error}=1-Precisión\n$$\n\nHasta 2025, no hay una función integrada para la tasa de malclasificación, al menos que yo sepa. Si encuentras una, compártelo. Hasta entonces, aquí tienes cómo puedes crear tu propia función.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntasa_error <- function(TP, TN, FP, FN){\n    (FP + FN) / (TP + TN + FP + FN)\n}\n\n# Estos datos NO son los mismos del ejemplo anterior\ntasa_error(TP = 46, TN = 115, FP = 8, FN = 39)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2259615\n```\n\n\n:::\n:::\n\n\n\nLas métricas restantes también se pueden calcular restando métricas conocidas de uno.\n\n## Tasa de falsos positivos\n\nLa **tasa de falsos positivos** se puede calcular restando la especificidad de uno.\n\n$$\nFPR=1-Especificidad=\\frac{FP}{TN+FP}\n$$ {#eq-tasa_falsos_positivos}\n\nLa tasa de falsos positivos es el porcentaje de verdaderos negativos clasificados incorrectamente como positivos. En términos sencillos, es cuántas veces se etiqueta incorrectamente a personas sanas como enfermas.\n\nPara calcular la tasa de falsos positivos a mano, solo necesitas la columna derecha de la matriz de confusión. Y para hacerlo aún más rápido, puedes usar la función de `epi.tests()` del paquete `epiR`, incluso con intervalos de confianza del 95% como bonificación.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(myfinance)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n```\n\n\n:::\n\n```{.r .cell-code}\nmodelo <- main_analysis()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Iniciando el análisis principal...\"\n[1] \"Cargando datos...\"\n[1] \"Datos cargados exitosamente. Número de filas: 4594\"\n[1] \"Creando variables predictoras...\"\n[1] \"Variables predictoras creadas exitosamente.\"\n[1] \"Creando tabla de datos...\"\n[1] \"Tabla de datos creada exitosamente.\"\n[1] \"Dividiendo datos en train y test...\"\n[1] \"Datos divididos exitosamente. Train: 3675 Test: 919\"\n[1] \"Normalizando datos...\"\n[1] \"Normalizados\"\n[1] \"Datos normalizados exitosamente.\"\n[1] \"Construyendo y entrenando el modelo...\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Modelo construido y entrenado exitosamente.\"\n[1] \"Haciendo predicciones para train...\"\n[1] \"Predicciones para train completadas.\"\n[1] \"Evaluando modelo en train...\"\n[1] \"Evaluación en train:\"\n[1] \"Haciendo predicciones para test...\"\n[1] \"Predicciones para test completadas.\"\n[1] \"Evaluando modelo en test...\"\n[1] \"Evaluación en test:\"\n[1] \"Análisis completado exitosamente.\"\n[1] \"Proceso finalizado.\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncalculate_results <- function(model, data) {\n  # Realizar predicción\n  predictions <- predict(model, data)\n  \n  # Convertir predicciones en probabilidades\n  probabilities <- 1 / (1 + exp(-predictions))\n  \n  # Determinar la dirección basada en el umbral de 0.5\n  direction <- ifelse(probabilities > 0.5, 1, 0)\n  \n  # Crear la matriz de confusión\n  confusion <- confusionMatrix(factor(direction), factor(data$Direction), mode = \"everything\")\n  \n  # Devolver los resultados\n  list(\n    predicted_direction = direction,\n    confusion_matrix = confusion,\n    predictions = predictions\n  )\n}\n\ntrain_results <- calculate_results(modelo$modelo, modelo$normalized$train)\ntrain_confusion <- train_results$confusion_matrix\n\nepiR::epi.tests(train_confusion$table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Outcome +    Outcome -      Total\nTest +         1152          108       1260\nTest -          134         2248       2382\nTotal          1286         2356       3642\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.35 (0.33, 0.36)\nTrue prevalence *                      0.35 (0.34, 0.37)\nSensitivity *                          0.90 (0.88, 0.91)\nSpecificity *                          0.95 (0.94, 0.96)\nPositive predictive value *            0.91 (0.90, 0.93)\nNegative predictive value *            0.94 (0.93, 0.95)\nPositive likelihood ratio              19.54 (16.24, 23.52)\nNegative likelihood ratio              0.11 (0.09, 0.13)\nFalse T+ proportion for true D- *      0.05 (0.04, 0.06)\nFalse T- proportion for true D+ *      0.10 (0.09, 0.12)\nFalse T+ proportion for T+ *           0.09 (0.07, 0.10)\nFalse T- proportion for T- *           0.06 (0.05, 0.07)\nCorrectly classified proportion *      0.93 (0.92, 0.94)\n--------------------------------------------------------------\n* Exact CIs\n```\n\n\n:::\n:::\n\n\n\nPara nuestro ejemplo:\n\n![FPR](images/epitest.png){#fig-2 fig-align=\"center\" width=\"451\"}\n\nPero aquí está el problema: la tasa de falsos positivos tiene muchos nombres en diferentes campos. Por ejemplo, en aprendizaje automático se le llama \"**falso positivo**\"; en estadísticas, **Error Tipo 1** o **probabilidad de falsa alarma**; en biometría, **tasa de coincidencia falsa**; en autenticación, **tasa de** **aceptación falsa**; en medicina y epidemiología, **fracción de falsos positivos**; en astronomía y astrofísica, **tasa de detección espuria**; e ingeniería, **nivel de ruido**. ¿Confuso, verdad? Pero no te preocupes, no importa cómo lo llames, la idea es la misma: **¿Cuántas veces cometemos errores al decir \"sí\" cuando la respuesta es realmente \"no\"?**\n\nPero, ¿por qué importa la tasa de falsos positivos? Porque las falsas alarmas **pueden ser costosas o incluso peligrosas**. Aquí hay algunos ejemplos del mundo real:\n\n-   Las falsas alarmas de incendio en Australia significan que los bomberos pueden correr a lugares donde no hay incendios, perdiendo tiempo y recursos, y posiblemente retrasando las respuestas a emergencias reales.\n\n-   En atención médica, una alta tasa de falsos positivos lleva a tratamientos innecesarios, como recetar medicamentos a pacientes sanos, lo que puede dañarlos y malgastar recursos médicos.\n\nSi hay una tasa de falsos positivos, también debe haber una tasa de falsos negativos, ¿verdad? Correcto. Vamos a sumergirnos en eso.\n\n## Tasa de falsos negativos\n\nLa tasa de falsos negativos es lo contrario de la tasa de falsos positivos. Mientras que la tasa de falsos positivos nos dice cuántas veces decimos \"sí\" cuando la respuesta es \"no\", la tasa de falsos negativos nos dice cuántas veces decimos \"no\" cuando la respuesta es realmente \"sí\". En términos sencillos, la **tasa de falsos negativos** mide el porcentaje de positivos reales clasificados incorrectamente como negativos, o cuántas veces se identifica incorrectamente a personas enfermas como sanas.\n\nAl igual que con la tasa de falsos positivos, la tasa de falsos negativos tiene muchos nombres confusos en diferentes campos. En medicina y epidemiología, se le llama \"**tasa de omisión\"** o \"**fracción de falsos negativos**\"; en estadísticas, **Error Tipo 2** o **probabilidad de omisión**; en aprendizaje automático, **1-Sensibilidad** o **1-Recall**. Y hay más nombres, lo cual es molesto. Pero todos se reducen a una sola idea: **¿Cuántas veces fallamos al detectar algo que realmente está allí?**\n\nPara calcular la tasa de falsos negativos, nos enfocamos en la columna izquierda de la matriz de confusión. Otra forma es restar 1-Sensibilidad o dividir el número de falsos negativos entre la suma de falsos negativos y verdaderos positivos.\n\n$$\nFNR=1-Sensibilidad=\\frac{FN}{FN+TP}\n$$ {#eq-tasa_falsos_negativos}\n\nAl igual que con la tasa de falsos positivos, podemos calcular la tasa de falsos negativos y sus intervalos de confianza del 95% usando la función de `epi.tests()`.\n\n![FNR](images/FNR.png){#fig-3 fig-align=\"center\" width=\"451\"}\n\nAquí está la parte alarmante: la tasa de falsos negativos, o **Error Tipo 2**, **a menudo es mucho peor** que la tasa de falsos positivos o Error Tipo 1. Déjame explicarte:\n\nImagina que te diagnostican cáncer, pero después de tres pruebas más, el diagnóstico es desestimado. Celebras, ¡estás absolutamente sano! Ese es un Error Tipo 1, aterrador pero no fatal.\n\nAhora imagina esto: tu primera prueba da negativo, así que celebras, pero más tarde empiezas a sentirte mal. Tres nuevas pruebas confirman que tienes cáncer. Además, debido a esa falsa negativo inicial, el cáncer ha progresado de forma irreversible. Ese es un **Error Tipo 2, más aterrador y literalmente fatal**. Por eso necesitamos descubrir cualquier falsa negativo lo antes posible.\n\n## Tasa descubrimiento falso\n\nY hablando de descubrimientos falsos, la siguiente métrica crucial que todo científico de datos debe conocer es la tasa de descubrimiento falso. En pocas palabras, la **tasa de descubrimiento falso** es el porcentaje de predicciones positivas que son realmente falsos positivos entre todas las predicciones positivas. En términos médicos, es el porcentaje de individuos sanos identificados incorrectamente como enfermos.\n\nPara calcular la tasa de descubrimiento falso, nos enfocamos en la fila superior de la matriz de confusión, ya que las predicciones están en las filas. Puedes computarla restando la precisión de uno o dividiendo el número de falsos positivos entre la suma de falsos positivos y verdaderos positivos.\n\n$$\nFDR=1-Precisión=\\frac{FP}{TP+FP}\n$$ {#eq-tasa_descubriemiento_falso}\n\nEn nuestro ejemplo, una tasa de descubrimiento falso del 9% significa que el 9% de todas las predicciones positivas fueron incorrectas. No está mal, pero aún hay espacio para mejorar. Nuestra confiable función `epi.tests()` también puede proporcionar la tasa de descubrimiento falso con intervalos de confianza exactos del 95%.\n\n![FDR](images/FDR.png){#fig-4 fig-align=\"center\" width=\"451\"}\n\nLa tasa de descubrimiento falso juega un papel significativo en técnicas de pruebas múltiples, como correcciones de **Bonferroni** o **valores de ajuste de Tukey** ajustan los $p-values$ cuando se realizan múltiples pruebas para controlar los errores.\n\nAquí está algo interesante: la tasa de descubrimiento falso mide la tasa a la que las predicciones positivas son realmente falsas. Así que la tasa de descubrimiento falso es simplemente lo contrario de cuando las predicciones positivas son correctas. Y puesto que la métrica donde las predicciones positivas son correctas se llama `valor predictivo positivo` (**PPV**), la FDR también se puede describir como lo contrario del valor predictivo positivo.\n\n$$\nFDR=1-PPV\n$$\n\nMientras que el valor predictivo positivo nos dice cuán a menudo una predicción positiva es correcta, la tasa de descubrimiento falso nos dice cuán a menudo es incorrecta. Y si existe un opuesto del valor predictivo positivo, podrías suponer que también existe un opuesto del valor predictivo negativo, ¿verdad? A eso se le llama la tasa de omisión falsa, y nos sumergiremos en ella a continuación, la última pieza del rompecabezas para entender cómo maneja tu modelo los errores.\n\n## Tasa de omisión falsa\n\nLa **tasa de omisión falsa** mide la proporción de predicciones negativas que son incorrectas, lo que indica cuán confiables son esas predicciones. Por ejemplo, en pruebas médicas, la tasa de omisión falsa nos dice el porcentaje de personas enfermas clasificadas incorrectamente como sanas. En pocas palabras, la tasa de omisión falsa responde la pregunta: **Cuando tu modelo predice algo negativo, ¿cuántas veces está realmente equivocado?**\n\nPara calcular la tasa de omisión falsa, nos enfocamos en la fila inferior de la matriz de confusión, donde están las predicciones negativas. Puedes computarla restando el valor predictivo negativo de uno o dividiendo el número de falsos negativos entre la suma de falsos negativos y verdaderos negativos.\n\n$$\nFOR=1-NPV=\\frac{FN}{TN+FN}\n$$ {#eq-tasa_omisión_falsa}\n\nEn nuestro ejemplo, el 25% de todas las predicciones negativas fueron incorrectas. Eso es un gran problema, especialmente en situaciones donde perder un caso positivo pueda tener consecuencias graves. La tasa de omisión falsa también se puede calcular usando la función `epi.tests()`.\n\n![FOR](images/FOR.png){#fig-5 fig-align=\"center\" width=\"451\"}\n\nAhora que hemos cubierto todas las métricas clave de error, es probable que te preguntes: `¿Hay una manera de combinar estas ideas en una sola medida de lo bueno que es realmente una prueba o un modelo?` Y la respuesta es sí. Se llama `razones de verosimilitud`: la razón de verosimilitud positiva y la razón de verosimilitud negativa. Juntas, te dan una imagen completa del poder diagnóstico de tu modelo y **son especialmente útiles para conjuntos de datos desequilibrados, donde métricas tradicionales como precisión o recall pueden fallar**. De hecho, estas métricas son tan esenciales que le he dedicado otro post. Confía en mí, no quieres perdértelo.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}