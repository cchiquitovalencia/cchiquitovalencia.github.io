{
  "hash": "990ba0dea3f093138670123bfa10b801",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Modelando stocks financieros\"\ndescription: \"TRADING Serie - Parte 1\"\nauthor:\n  - name: Cristian Chiquito Valencia\n    url: https://cchiquitovalencia.github.io/\n    affiliation: Independent @ CHV\ndate: 04-07-2025\ncategories: [Machine Learning, Trading, Regresión Logística] # self-defined categories\ncitation: \n  url: https://cchiquitovalencia.github.io/posts/2025-04-07-predictions_on_stocks/\nimage: stocks1.jpeg\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n---\n\n::: {.cell}\n\n:::\n\n\n\nLos modelos de **machine learning** hacen predicciones que caen en cuatro resultados posibles:\n\n![Matriz de confusión](confusionMatrix.png){#fig-1 fig-align=\"center\"}\n\n-   Verdadero positivo (**True Positive**): El modelo predice correctamente la clase positiva.\n\n-   Falso positivo (**False Positive**): El modelo predice incorrectamente la clase positiva cuando el resultado real es negativo, también conocido como error de tipo I.\n\n-   Verdadero negativo (**True Negative**): El modelo predice correctamente la clase negativa.\n\n-   Falso negativo (**False Negative)**: El modelo predice incorrectamente la clase negativa cuando el resultado real es positivo, también conocido como error de tipo II.\n\nEstos resultados forman la base para evaluar el rendimiento de un modelo.\n\nLa presencia de falsos positivos y falsos negativos resalta dónde las predicciones del modelo fallan, o, en palabras más sencillas, dónde se confunden los valores. Por eso, la tabla 2x2 utilizada para organizar estos resultados ha ganado el impresionante título de `matriz de confusión`.\n\nLo que realmente es notable es que, a pesar de contener solo cuatro números, una matriz de confusión puede generar **al menos 27 métricas** distintas que miden el poder predictivo. Cada métrica tiene un propósito único, dependiendo del contexto en el que se aplique. Dado el gran número de métricas, no puedo incluirlas todas en un solo post, así que a lo largo de esta seria las veremos. En cada post, exploraré un grupo de métricas relacionadas, explicando qué significan, demostrando cómo calcularlas en R y proporcionando ejemplos del mundo real de dónde y por qué son importantes.\n\nAhora, sumérjamonos y creemos una matriz de confusión a partir de las predicciones del modelo.\n\nPara construir una matriz de confusión para un modelo de \\*machine learning\\*, sigue estos pasos:\n\n1.  Divide tu conjunto de datos en dos partes, aproximadamente un `80%` para entrenamiento y un `20%` para prueba.\n\n2.  Usa solo los datos de entrenamiento para `enseñar` al modelo.\n\n3.  Usa el modelo entrenado para `predecir` las probabilidades del resultado positivo para los datos de prueba.\n\n4.  Luego, convierte las probabilidades en categorías, es decir, si la probabilidad es `mayor a 0.5`, clasifícala como \"sí\", y si es `0.5 o menor,` clasifícala como \"no\".\n\n5.  Finalmente, compara los valores reales en los datos de prueba con los predichos utilizando una sencilla `tabla 2x2 llamada matriz de confusión`.\n\nAquí vamos a usar datos de stocks financieros, vamos a construir un modelo de predicción que nos indique la dirección del mercado. La dirección del mercado es muy importante para los inversores o traders. Predecir la dirección del mercado es una tarea bastante desafiante, ya que los datos del mercado incluyen mucho ruido. El mercado se mueve hacia arriba o hacia abajo, y la naturaleza del movimiento del mercado es binario. Un modelo de regresión logística nos ayuda a ajustar un modelo usando comportamiento binario y predecir la dirección del mercado. La regresión logística es uno de los modelos probabilísticos que asigna una probabilidad a cada evento.\n\nLo primero es conseguir la información de algún índice bursátil, digamos **DJI (Dow Jones Industrial)**, que refleja el comportamiento del precio de la acción de las treinta compañías industriales más importantes y representativas de Estados Unidos. Para esto usamos la librería quantmod, y extraemos el precio de cierre:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"quantmod\")\ngetSymbols(\"^DJI\",src=\"yahoo\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"DJI\"\n```\n\n\n:::\n\n```{.r .cell-code}\ndji<- DJI[,\"DJI.Close\"]\n```\n:::\n\n\n\nVamos a agreagar al modelo más variables: el promedio de los 10 días anteriores, el promedio de 20 días anteriores y sus desviaciones estándar, el `RSI`, el `MACD` y las `Bollinger Bands`. Esas variables harán parte de las variables independientes del modelo, la dirección será nuestra variable dependiente, la que queremos predecir.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg10<- rollapply(dji,10,mean)\navg20<- rollapply(dji,20,mean)\nstd10<- rollapply(dji,10,sd)\nstd20<- rollapply(dji,20,sd)\nrsi5<- RSI(dji,5,\"SMA\")\nrsi14<- RSI(dji,14,\"SMA\")\nmacd12269<- MACD(dji,12,26,9,\"SMA\")\nmacd7205<- MACD(dji,7,20,5,\"SMA\")\nbbands<- BBands(dji,20,\"SMA\",2)\n```\n:::\n\n\n\nAhora verificamos si hubo cambio en la dirección, y armamos el conjunto de datos completo. Haz una pausa en el código, fíjate que nuestra decisión depende de la evaluación respecto del valor 20 días atrás:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDirection<- NULL\nDirection[dji> Lag(dji,20)] <- 1\nDirection[dji< Lag(dji,20)] <- 0\n\ndji<-cbind(dji,\n           avg10,\n           avg20,\n           std10,\n           std20,\n           rsi5,\n           rsi14,\n           macd12269,\n           macd7205,\n           bbands,\n           Direction)\n\ndji$Direction <- ifelse(is.na(dji$Direction), 0, dji$Direction)\n```\n:::\n\n\n\nAhora, para implementar `regresión logística`, debemos dividir los datos en dos partes. La primera parte son los datos en muestra y la segunda parte son los datos fuera de muestra.\n\nLos datos en muestra se utilizan para el proceso de construcción del modelo y los datos fuera de muestra se utilizan para fines de evaluación. Este proceso también ayuda a controlar la varianza y el sesgo en el modelo. Las siguientes cuatro líneas corresponden a las fechas de inicio y fin de los datos en muestra y los datos fuera de muestra, respectivamente.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfechas <- as.data.frame(dji) |> \n  row.names()\n\nissd<- first(fechas[1:(length(fechas) * 0.8)])\nised<- last(fechas[1:(length(fechas) * 0.8)])\nossd<- as.Date(ised) + 1\nosed<- last(fechas)\n```\n:::\n\n\n\nLos siguientes dos comandos tienen como objetivo obtener el número de fila para las fechas, es decir, la variable `isrow` extrae los números de fila para el rango de fechas en muestra y `osrow` extrae los números de fila para el rango de fechas fuera de muestra.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nisrow<- which(index(dji) >= issd & index(dji) <= ised)\nosrow<- which(index(dji) >= ossd & index(dji) <= osed)\n```\n:::\n\n\n\nLas variables `isdji` y `osdji` son conjuntos de datos en muestra y fuera de muestra, respectivamente.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nisdji<- dji[isrow,]\nosdji<- dji[osrow,]\n```\n:::\n\n\n\nSi miras los datos en muestra, es decir, `isdji`, te darás cuenta de que la escala de cada columna es diferente: algunos columnas están en una escala de 100, otros están en una escala de 10,000 y algunos otros están en una escala de 1. La diferencia en la escala puede causar problemas en tus resultados, ya que se asignan pesos más altos a variables escaladas más altas. Por lo tanto, antes de seguir adelante, debes considerar estandarizar el conjunto de datos. Voy a utilizar la siguiente fórmula:\n\n$$\n\\text{Datos estandarizados}=\\frac{X - Media(X)}{\\text{Desviación Estándar}(X)}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nisme <- apply(isdji, 2, function(x) mean(x, na.rm = TRUE))\nisstd <- apply(isdji, 2, function(x) sd(x, na.rm = TRUE))\n```\n:::\n\n\n\nSe genera una matriz identidad de dimensión igual a los datos en muestra utilizando el siguiente comando, que se va a utilizar para la normalización.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nisidn<- matrix(1,dim(isdji)[1],dim(isdji)[2])\nnorm_isdji<-  (isdji - t(isme*t(isidn))) / t(isstd*t(isidn))\n```\n:::\n\n\n\nLa línea anterior también estandariza la columna de `Dirección`, es decir, la columna última. No queremos que la dirección se estandarice, por lo que reemplazo la columna última nuevamente con la variable dirección para el rango de datos en muestra.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndm<- dim(isdji)\nnorm_isdji[,dm[2]] <- Direction[isrow]\n```\n:::\n\n\n\nAhora hemos creado todos los datos necesarios para construir el modelo. Debe construir un modelo de regresión logística y esto nos ayudará a predecir la dirección del mercado basado en los datos en muestra.\n\nEn primer lugar, en este paso, creé una fórmula que tiene la dirección como variable dependiente y todas las demás columnas como variables independientes. Luego, utilicé un modelo lineal generalizado, es decir, `glm()`, para ajustar un modelo que tiene fórmula, familia y conjunto de datos:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula<- paste(\"Direction ~ .\",sep=\"\")\n\nmodel<- glm(formula,family=\"binomial\",norm_isdji)\n```\n:::\n\n\n\nSe puede ver un resumen del modelo utilizando el siguiente comando:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = formula, family = \"binomial\", data = norm_isdji)\n\nCoefficients: (3 not defined because of singularities)\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.09753    0.11965   9.173  < 2e-16 ***\nDJI.Close    32.69491    5.17170   6.322 2.58e-10 ***\nDJI.Close.1 -34.09386   11.02876  -3.091 0.001992 ** \nDJI.Close.2   1.53962   10.73311   0.143 0.885938    \nDJI.Close.3   0.07123    0.24846   0.287 0.774344    \nDJI.Close.4  -0.29423    0.31673  -0.929 0.352911    \nrsi           0.04493    0.15774   0.285 0.775776    \nrsi.1        -1.35814    0.23602  -5.754 8.70e-09 ***\nmacd          3.28435    0.83366   3.940 8.16e-05 ***\nsignal        1.34016    0.35251   3.802 0.000144 ***\nmacd.1       -0.67821    0.48475  -1.399 0.161790    \nsignal.1      2.05191    0.92635   2.215 0.026756 *  \ndn                 NA         NA      NA       NA    \nmavg               NA         NA      NA       NA    \nup                 NA         NA      NA       NA    \npctB          2.35390    0.25878   9.096  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4729.8  on 3641  degrees of freedom\nResidual deviance: 1218.0  on 3629  degrees of freedom\n  (33 observations deleted due to missingness)\nAIC: 1244\n\nNumber of Fisher Scoring iterations: 8\n```\n\n\n:::\n:::\n\n\n\nUsamos la función `predict()` para ajustar valores en el mismo conjunto de datos para estimar el mejor valor ajustado.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred<- predict(model,norm_isdji)\n```\n:::\n\n\n\nUna vez que hayas ajustado los valores, debes intentar convertirlos a probabilidades utilizando el siguiente comando. Esto convertirá la salida en forma probabilística y la salida será en el rango $[0,1]$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprob<- 1 / (1+exp(-(pred)))\n```\n:::\n\n\n\nLa primera línea del código muestra que dividimos la figura en dos filas y una columna, donde la primera figura es para la predicción del modelo y la segunda figura es para la probabilidad.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,1))\nplot(pred,type=\"l\")\nplot(prob,type=\"l\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=768}\n:::\n:::\n\n\n\nComo las probabilidades están en el rango $(0,1)$, así también lo está nuestro vector prob. Ahora, para clasificarlos en una de los dos clases, consideré la dirección hacia arriba $(1)$ cuando `prob` es mayor que $0.5$ y la dirección hacia abajo $(0)$ cuando `prob` es menor que $0.5$. Esta asignación se puede hacer utilizando los siguientes comandos:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_direction<- NULL\npred_direction[prob> 0.5] <- 1\npred_direction[prob<= 0.5] <- 0\n```\n:::\n\n\n\nUna vez que hemos determinado la dirección predicha, debemos verificar la precisión del modelo: cuánto ha predicho la dirección hacia arriba como hacia arriba y la dirección hacia abajo como hacia abajo. Es posible que haya algunos escenarios en los que predijo lo contrario de lo que es, como predijo hacia abajo cuando en realidad es hacia arriba y viceversa. Puedemos utilizar el paquete `caret` para calcular `confusionMatrix()`, que devuelve una matriz como salida. Todos los elementos diagonales son predichos correctamente y los elementos fuera de la diagonal son errores o predichos incorrectamente. Debe tener como objetivo reducir los elementos fuera de la diagonal en una matriz de confusión.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nmatrix<- confusionMatrix(factor(pred_direction),\n                         factor(norm_isdji$Direction),\n                         mode = \"everything\")\nmatrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1152  108\n         1  134 2248\n                                         \n               Accuracy : 0.9336         \n                 95% CI : (0.925, 0.9414)\n    No Information Rate : 0.6469         \n    P-Value [Acc > NIR] : <2e-16         \n                                         \n                  Kappa : 0.8539         \n                                         \n Mcnemar's Test P-Value : 0.108          \n                                         \n            Sensitivity : 0.8958         \n            Specificity : 0.9542         \n         Pos Pred Value : 0.9143         \n         Neg Pred Value : 0.9437         \n              Precision : 0.9143         \n                 Recall : 0.8958         \n                     F1 : 0.9049         \n             Prevalence : 0.3531         \n         Detection Rate : 0.3163         \n   Detection Prevalence : 0.3460         \n      Balanced Accuracy : 0.9250         \n                                         \n       'Positive' Class : 0              \n                                         \n```\n\n\n:::\n:::\n\n\n\nTen cuidado, sin embargo, porque aunque una matriz de confusión solo tiene cuatro números, es fácil confundirse. Esto es lo que debes recordar: las filas deben representar los valores predichos por tu modelo o datos de prueba. La fila superior es para predicciones positivas, y la fila inferior es para predicciones negativas. Las columnas deben representar los valores reales o verdaderos, también llamados el \"estándar de oro\". La columna izquierda es para resultados positivos, mientras que la columna derecha es para resultados negativos.\n\nLa función `epiR::epi.tests()` tiene dos características útiles: te recuerda colocar los resultados reales o los valores reales en las columnas y proporciona intervalos de confianza al $95\\%$, algo que muchas funciones comunes de **machine learning** como la matriz de confusión del paquete `caret::confusionMatrix()` no incluyen. Dicho esto, la función `confusionMatrix` también tiene ventajas. Por ejemplo, incluye métricas como la tasa de detección (`Detection Rate`), que es un excelente punto de partida para analizar el rendimiento del modelo porque es simple e intuitiva.\n\nLa tabla precedente muestra que hemos logrado una predicción correcta del 93%, ya que 1152 + 2248 = 3400 predicciones son correctas de un total de 3642 (suma de todos los cuatro valores). En general, se considera una buena predicción cualquier valor superior al $80\\%$ en los datos de muestra; sin embargo, el $80\\%$ no es un valor fijo y debe determinarse en función del conjunto de datos y la industria.\n\nAhora que has implementado el modelo de regresión logística, que ha predicho correctamente el 93%, debes probar su capacidad de generalización. Debe probarse este modelo utilizando datos de muestra y verificar su precisión.\n\nEl primer paso es estandarizar los datos de muestra utilizando la fórmula de arriba. En este caso, la media y la desviación estándar deben ser las mismas que las utilizadas para la normalización de los datos de muestra.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nosidn<- matrix(1,dim(osdji)[1],dim(osdji)[2])\nnorm_osdji<-  (osdji - t(isme*t(osidn))) / t(isstd*t(osidn))\nnorm_osdji[,dm[2]] <- Direction[osrow]\n```\n:::\n\n\n\nA continuación, utilizaremos la función `predict()` en los datos de muestra y utilizaremos este valor para calcular la probabilidad.\n\nUna vez que se hayan determinado las probabilidades para los datos de muestra, debes clasificarlos en clases \"**Arriba**\" o \"**Abajo**\" utilizando los siguientes comandos. La función `confusionMatrix()` aquí generará una matriz para los datos de muestra.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nospred<- predict(model,norm_osdji)\nosprob<- 1 / (1+exp(-(ospred)))\nospred_direction<- NULL\nospred_direction[osprob> 0.5] <- 1\nospred_direction[osprob<= 0.5] <- 0\nosmatrix<- confusionMatrix(factor(ospred_direction),\n                           factor(norm_osdji$Direction))\nosmatrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 369  37\n         1  26 487\n                                          \n               Accuracy : 0.9314          \n                 95% CI : (0.9131, 0.9469)\n    No Information Rate : 0.5702          \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.8606          \n                                          \n Mcnemar's Test P-Value : 0.2077          \n                                          \n            Sensitivity : 0.9342          \n            Specificity : 0.9294          \n         Pos Pred Value : 0.9089          \n         Neg Pred Value : 0.9493          \n             Prevalence : 0.4298          \n         Detection Rate : 0.4015          \n   Detection Prevalence : 0.4418          \n      Balanced Accuracy : 0.9318          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n\n\n:::\n:::\n\n\n\nEsto muestra un 93% de precisión en los datos de muestra. La calidad de la precisión es más allá del alcance de este post, por lo que no cubriré si la precisión en los datos de muestra es buena o mala y qué técnicas se pueden utilizar para mejorar este rendimiento.\n\nUn modelo de trading realista también tiene en cuenta los costos de trading y la pérdida de mercado, lo que disminuye significativamente las probabilidades de ganar. La próxima tarea es diseñar una estrategia de trading utilizando las direcciones predichas. Exploraré cómo implementar una estrategia de trading automatizada utilizando señales predichas en otro post.\n\nPor ahora, es importante que conozcas que la tasa de detección nos dice la proporción de positivos reales que el modelo identificó correctamente. En nuestro ejemplo, la tasa de detección es del 0.3163097, pero ¿es esto bueno o malo? Depende de la situación. En escenarios de alto riesgo, como la detección de enfermedades o la prevención de fraudes, un 32% probablemente sea demasiado bajo, ya que se pierden muchos verdaderos positivos, lo que puede tener consecuencias graves.\n\nEn problemas desafiantes, como aquellos con conjuntos de datos desequilibrados o tareas complejas, un 32% podría representar realmente un progreso sólido. Por ejemplo, la tasa de detección es especialmente útil en fabricación para monitorear las tasas de defectos (piensa en Six Sigma y las partes por millón). Ayuda a identificar productos defectuosos en fábricas modernas operadas por robots, donde incluso mejoras pequeñas pueden marcar una gran diferencia. Sin embargo, la tasa de detección tiene una limitación importante: solo se enfoca en los verdaderos positivos e ignora completamente los falsos positivos. Esto significa que no nos da una visión completa de cómo está funcionando el modelo. Para obtener una comprensión más completa, necesitamos mirar la siguiente métrica importante en la matriz de confusión: la prevalencia de detección (`Detection Prevalence`).\n\nLa prevalencia de detección, también conocida como prevalencia aparente, es el porcentaje de casos positivos que el modelo predice. En nuestro ejemplo del finanzas, representa la proporción de veces que el modelo predice que el precio de la acción bajará. Así es como la calculamos: sumamos todos los positivos predichos, que son los valores en la fila superior de la matriz de confusión, y los dividimos entre el número total de casos. Esto significa que nuestro modelo predice que el 0.3459638 del precio de la acción bajará. ¿Por qué se llama prevalencia aparente? El término refleja el hecho de que estas son predicciones modeladas que pueden no alinearse con la realidad. Para entender la realidad, necesitamos calcular la prevalencia verdadera, que exploraremos a continuación.\n\nLa prevalencia verdadera muestra el porcentaje real de casos positivos, incluyendo aquellos que el modelo se perdió. En el ejemplo de stocks, es la proporción real de subidas de la acción, incluso aquellos a los que el modelo predijo incorrectamente que no subían. Para calcular la prevalencia verdadera, nos enfocamos en la columna izquierda de la matriz de confusión, sumamos los verdaderos positivos y los falsos negativos, y los dividimos entre el tamaño de la población.\n\nEs tan simple como eso. La prevalencia verdadera es del 35.31%, lo que es mayor que la prevalencia aparente predicha por el modelo, que es del 34.60%. Esta es una brecha enorme, especialmente en casos sensibles, los que involucran vidas humanas. Esto nos lleva naturalmente a la sensibilidad, una métrica fundamental para entender cómo bien identifica un modelo los verdaderos positivos.\n\nLa sensibilidad es solo el comienzo. Hay siete métricas esenciales de rendimiento que forman la base de la evaluación de cualquier modelo de machine learning. En el próximo post, pasaré por cada una de ellas paso a paso.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}